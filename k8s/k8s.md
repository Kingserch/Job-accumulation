+ ### Kubernetes
    + [环境准备](#环境准备)
    + [DNS服务初始化](#DNS服务初始化)
		    + [在hdss7-11主机上安装bind服务](#在hdss7-11主机上安装bind服务)
    + [配置证书](#配置证书)
    + [安装docker并搭建harbor](#安装docker)
    + [部署master节点服务(etcd证书，etcd集群)](#部署master节点服务)
    + [部署kube-apiserver集群](#部署kube-apiserver集群)
    + [配置nginx反向代理L4,keepalived反向代理](#nginx四层反向代理)
	+ [部署主控制节点控制器和调度控制器服务](#在129和130机器上)
	+ [部署运算节点服务kubelet](#部署运算节点服务kubelet)
+ ### 环境准备
`yum install epel-release -y`   
`yum install wget net-tools telnet tree nmap sysstat lrzsz dos2unix bind-utils -y`  
+ ### DNS服务初始化
+ ##### 在hdss7-11主机上安装bind服务
```
#安装bind服务，因为要用ingress，在k8s里面要做7层流量调度，要绑定域名。(k8s容器绑定host，)
[root@hdss7-129 ~]# yum install bind -y
[root@hdss7-129 ~]# rpm -qa bind
bind-9.11.4-16.P2.el7_8.2.x86_64
[root@hdss7-129 ~]# netstat   -rn   #查看网关
Kernel IP routing table
Destination     Gateway         Genmask         Flags   MSS Window  irtt Iface
0.0.0.0         192.168.56.2    0.0.0.0         UG        0 0          0 ens33
172.7.68.0      0.0.0.0         255.255.255.0   U         0 0          0 docker0
192.168.56.0    0.0.0.0         255.255.255.0   U         0 0          0 ens33
[root@hdss7-129 ~]# vi /etc/named.conf
options {
        listen-on port 53 { 192.168.56.129; };	#默认是127.0.0.1本机需要改成本机的内网ip
        directory       "/var/named";
        dump-file       "/var/named/data/cache_dump.db";
        statistics-file "/var/named/data/named_stats.txt";
        memstatistics-file "/var/named/data/named_mem_stats.txt";
        recursing-file  "/var/named/data/named.recursing";
        secroots-file   "/var/named/data/named.secroots";
        allow-query     { any; };
        forwarders      { 0.0.0.0; };	#本机的网关
        recursion yes;	#一定要是yes 表示用递归的算法查询DNS，另一种是迭代
        dnssec-enable no;
        dnssec-validation no;
[root@hdss7-129 /]# named-checkconf	#检查
```
##### 2)在hdss7-129主机配置区域配置文件
```
[root@hdss7-129 /]# vim /etc/named.rfc1912.zones
...#在最后添加下面字段
zone "host.com" IN {
        type  master;
        file  "host.com.zone";
        allow-update { 192.168.56.129; };
};

zone "od.com" IN {
        type  master;
        file  "od.com.zone";
        allow-update { 192.168.56.129; };
};
```
##### 3)在hdss7-129主机编辑区域数据文件
```
[root@hdss7-129 /]# vim /var/named/host.com.zone
$ORIGIN host.com.
$TTL 600	; 10 minutes
@       IN SOA	dns.host.com. dnsadmin.host.com. (
				2020051101 ; serial
				10800      ; refresh (3 hours)
				900        ; retry (15 minutes)
				604800     ; expire (1 week)
				86400      ; minimum (1 day)
				)
			NS   dns.host.com.
$TTL 60	; 1 minute
dns                A    192.168.56.129
hdss7-129           A    192.168.56.129
hdss7-128           A    192.168.56.128
hdss7-130           A    192.168.56.130
hdss7-132          A    192.168.56.132
[root@hdss7-129 /]# vim /var/named/od.com.zone	业务域
$ORIGIN od.com.
$TTL 600	; 10 minutes
@   		IN SOA	dns.od.com. dnsadmin.od.com. (
				2020051101 ; serial
				10800      ; refresh (3 hours)
				900        ; retry (15 minutes)
				604800     ; expire (1 week)
				86400      ; minimum (1 day)
				)
				NS   dns.od.com.
$TTL 60	; 1 minute
dns                A    192.168.56.129
[root@hdss7-129 /]# named-checkconf		#检查更改的配置文件
[root@hdss7-129 /]# systemctl start named	
[root@hdss7-129 /]# netstat -luntp |grep 53 
tcp        0      0 192.168.56.129:53       0.0.0.0:*               LISTEN      18812/named         
tcp        0      0 127.0.0.1:953           0.0.0.0:*               LISTEN      18812/named         
tcp6       0      0 ::1:953                 :::*                    LISTEN      18812/named         
udp        0      0 192.168.56.129:53       0.0.0.0:*                           18812/named   
[root@hdss7-129 ~]# dig -t A hdss7-128.host.com @192.168.56.129 +short
192.168.56.129
[root@hdss7-129 ~]# dig -t A hdss7-129.host.com @192.168.56.129 +short
192.168.56.128
[root@hdss7-129 ~]# dig -t A hdss7-130.host.com @192.168.56.129 +short
192.168.56.130
[root@hdss7-129 ~]# dig -t A hdss7-132.host.com @192.168.56.129 +short
192.168.56.132
[root@hdss7-129 ~]# cat /etc/resolv.conf 
# Generated by NetworkManager
search localdomain
nameserver 192.168.56.2
nameserver 192.168.56.129
```
+ ### 配置证书
```
[root@hdss7-132 ~]# wget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64 -O /usr/bin/cfssl
[root@hdss7-132 ~]# wget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64 -O /usr/bin/cfssl-json
[root@hdss7-132 ~]# wget https://pkg.cfssl.org/R1.2/cfssl-certsinfo_linux-amd64 -O /usr/bin/cfssl-certsinfo
[root@hdss7-132 ~]# chmod +x /usr/bin/cfssl*
[root@hdss7-132 ~]# which cfssl
/usr/bin/cfssl
[root@hdss7-132 ~]# which cfssl-json
/usr/bin/cfssl-json
[root@hdss7-132 ~]# which cfssl-certsinfo
/usr/bin/cfssl-certsinfo
[root@hdss7-132 ~]# mkdir -p /opt/certss
[root@hdss7-132 ~]# cd /opt/certss/
[root@hdss7-132 certss]# pwd
/opt/certss
[root@hdss7-132 certss]# vi ca-csr.json
{
    "CN": "kingEdu",
    "hosts": [
    ],
    "key": {
        "algo": "rsa",	#rsa加密算法
        "size": 2048	#长度2048
    },
    "names": [
        {
            "C": "CN",
            "ST": "beijing",
            "L": "beijing",
            "O": "od",
            "OU": "ops"
        }
    ],
    "ca": {
        "expiry": "175200h"		#证书过期时间
    }
}
[root@hdss7-132 certss]# cfssl gencerts -initca ca-csr.json | cfssl-json -bare ca
2020/05/11 16:53:28 [INFO] generating a new CA key and certsificate from CSR
2020/05/11 16:53:28 [INFO] generate received request
2020/05/11 16:53:28 [INFO] received CSR
2020/05/11 16:53:28 [INFO] generating key: rsa-2048
2020/05/11 16:53:28 [INFO] encoded CSR
2020/05/11 16:53:28 [INFO] signed certsificate with serial number 217885965496432947483112196286183500766512119525
[root@hdss7-132 certss]# ll
total 16
-rw-r--r-- 1 root root  993 May 11 16:53 ca.csr
-rw-r--r-- 1 root root  326 May 11 16:49 ca-csr.json
-rw------- 1 root root 1679 May 11 16:53 ca-key.pem		#根证书的私钥
-rw-r--r-- 1 root root 1338 May 11 16:53 ca.pem			#根证书的私钥
```
+ ### 安装docker
```
curl -fsSL https://get.docker.com | bash -s docker --mirror Aliyun
#每台机器都安装docker
[root@hdss7-132 docker]# cat daemon.json
{
  "graph": "/data/docker",
  "storage-driver": "overlay2",
  "insecure-registries": ["registry.access.redhat.com","quay.io","harbor.od.com"],		#添加harbor.od.com字段，把仓库设置为可信任
  "registry-mirrors": ["https://q2gr04ke.mirror.aliyuncs.com"],
  "bip": "172.7.132.1/24",	#bip 取linux主机的ip后一位，方便管理
  "exec-opts": ["native.cgroupdriver=systemd"],
  "live-restore": true
}
https://github.com/goharbor/harbor  #使用harbor搭建docker私有仓库
[root@hdss7-132]# mkdir /opt/src -p
[root@hdss7-132 src]# pwd
/opt/src
[root@hdss7-132 src]# tar xvf harbor-offline-installer-v1.8.3.tgz -C/opt 
harbor/harbor.v1.8.3.tar.gz
harbor/prepare
harbor/LICENSE
harbor/install.sh
harbor/harbor.yml
[root@hdss7-132 opt]# mv harbor/ harbor-v1.8.3
[root@hdss7-132 opt]# ln -s /opt/harbor-v1.8.3/ /opt/harbor	#软连接到/opt/src下，方便以后升级
[root@hdss7-132 opt]# ll
total 0
drwxr-xr-x  2 root root  71 May 11 16:53 certss
drwx--x--x. 4 root root  28 May 11 12:06 containerd
lrwxrwxrwx  1 root root  19 May 11 17:37 harbor -> /opt/harbor-v1.8.3/
drwxr-xr-x  2 root root 100 May 11 17:36 harbor-v1.8.3
drwxr-xr-x  2 root root  49 May 11 17:34 src
[root@hdss7-132 harbor]# vim harbor.yml
hostname: harbor.od.com
   port: 180
harbor_admin_password: Harbor12345
data_volume: /data/docker
#安装docker-compose，harbor基于单机编排
yum install docker-compose -y
[root@hdss7-132 harbor]# rpm -qa docker-compose
docker-compose-1.18.0-4.el7.noarch
[root@hdss7-132 harbor]# ./install.sh
[root@hdss7-132 harbor]# yum install nginx -y  #使用nginx反代
[root@hdss7-132 harbor]# vim /etc/nginx/conf.d/harbor.od.com.conf
server {
    listen       80;
    server_name  harbor.od.com;

    client_max_body_size 1000m;

    location / {
        proxy_pass http://127.0.0.1:180;
    }
}
[root@hdss7-132 harbor]# systemctl restart nginx
hdss7-129上:
[root@hdss7-129 ~]# vi /var/named/od.com.zone
harbor             A    192.168.56.132
```
浏览器打开http://harbor.od.com
![](https://github.com/Kingserch/Job-accumulation/blob/Kubernetes/images/harbor.png)
```
[root@hdss7-132 conf.d]# docker pull nginx:1.7.9
[root@hdss7-132 conf.d]# docker tag 84581e99d807 harbor.od.com/public/nginx:v1.7.9
[root@hdss7-132 conf.d]# docker login harbor.od.com
Username: admin
Password: 
WARNING! Your password will be stored unencrypted in /root/.docker/config.json.
Configure a credential helper to remove this warning. See
https://docs.docker.com/engine/reference/commandline/login/#credentials-store

Login Succeeded
[root@hdss7-132 ~]# docker push harbor.od.com/public/nginx:v1.7.9
The push refers to repository [harbor.od.com/public/nginx]
5f70bf18a086: Pushed 
4b26ab29a475: Pushed 
ccb1d68e3fb7: Pushed 
e387107e2065: Pushed 
63bf84221cce: Pushed 
e02dce553481: Pushed 
dea2e4984e29: Pushed 
v1.7.9: digest: sha256:b1f5935eb2e9e2ae89c0b3e2e148c19068d91ca502e857052f14db230443e4c2 size: 3012
```
+ ### 部署master节点服务

#### 1)配置etcd证书
```
[root@hdss7-132 certss]# vim /opt/certss/ca-config.json
{
    "signing": {
        "default": {
            "expiry": "175200h"	#证书过期时间
        },
        "profiles": {
            "server": {
                "expiry": "175200h",
                "usages": [
                    "signing",
                    "key encipherment",
                    "server auth"
                ]
            },
            "client": {		#客户端
                "expiry": "175200h",
                "usages": [
                    "signing",
                    "key encipherment",
                    "client auth"
                ]
            },
            "peer": {			#互相通信
                "expiry": "175200h",
                "usages": [
                    "signing",
                    "key encipherment",
                    "server auth",
                    "client auth"
                ]
            }
        }
    }
}
[root@hdss7-132 certss]# vi etcd-peer-csr.json
{
    "CN": "k8s-etcd",
    "hosts": [
        "192.168.56.128",
        "192.168.56.129",
        "192.168.56.130",
        "192.168.56.132"
    ],
    "key": {
        "algo": "rsa",
        "size": 2048
    },
    "names": [
        {
            "C": "CN",
            "ST": "beijing",
            "L": "beijing",
            "O": "od",
            "OU": "ops"
        }
    ]
}
#生产etcd证书和私钥
[root@hdss7-132 certss]# cfssl gencerts -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=peer etcd-peer-csr.json |cfssl-json -bare etcd-peer
2020/05/12 11:19:23 [INFO] generate received request
2020/05/12 11:19:23 [INFO] received CSR
2020/05/12 11:19:23 [INFO] generating key: rsa-2048
2020/05/12 11:19:23 [INFO] encoded CSR
2020/05/12 11:19:23 [INFO] signed certsificate with serial number 65897943636016508687974124617311544054963979665
2020/05/12 11:19:23 [WARNING] This certsificate lacks a "hosts" field. This makes it unsuitable for
websites. For more information see the Baseline Requirements for the Issuance and Management
of Publicly-Trusted certsificates, v.1.1.6, from the CA/Browser Forum (https://cabforum.org);
specifically, section 10.2.3 ("Information Requirements").
[root@hdss7-132 certss]# ll
total 36
-rw-r--r-- 1 root root  836 May 12 11:04 ca-config.json
-rw-r--r-- 1 root root  993 May 11 16:53 ca.csr
-rw-r--r-- 1 root root  326 May 11 16:49 ca-csr.json
-rw------- 1 root root 1679 May 11 16:53 ca-key.pem
-rw-r--r-- 1 root root 1338 May 11 16:53 ca.pem
-rw-r--r-- 1 root root 1062 May 12 11:19 etcd-peer.csr
-rw-r--r-- 1 root root  383 May 12 11:11 etcd-peer-csr.json
-rw------- 1 root root 1679 May 12 11:19 etcd-peer-key.pem
-rw-r--r-- 1 root root 1424 May 12 11:19 etcd-peer.pem
```
#### 2)部署etcd集群
https://github.com/etcd-io/etcd/tags
```
[root@hdss7-128 opt]# mkdir src
[root@hdss7-128 opt]# cd src
[root@hdss7-128 src]# useradd  -s /sbin/nologin -M etcd
[root@hdss7-128 src]# id etcd
uid=1001(etcd) gid=1001(etcd) groups=1001(etcd)
[root@hdss7-128 src]# tar xvf etcd-v3.1.20-linux-amd64.tar.gz -C /opt/ 
[root@hdss7-128 src]# cd /opt/
[root@hdss7-128 opt]# ls
containerd  etcd-v3.1.20-linux-amd64  src
[root@hdss7-128 opt]# mv etcd-v3.1.20-linux-amd64/ etcd-v3.1.20
[root@hdss7-128 opt]# ln -s /opt/etcd-v3.1.20/ /opt/etcd		#做个软连接方便以后升级
[root@hdss7-128 opt]# ll
total 0
drwx--x--x. 4 root   root   28 May 11 12:06 containerd
lrwxrwxrwx  1 root   root   18 May 12 11:41 etcd -> /opt/etcd-v3.1.20/
drwxr-xr-x  3 478493 89939 123 Oct 11  2018 etcd-v3.1.20
drwxr-xr-x  2 root   root   45 May 12 11:37 src
[root@hdss7-128 opt]# cd etcd
[root@hdss7-128 etcd]# mkdir -p /opt/etcd/certss /data/etcd /data/logs/etcd-server
[root@hdss7-128 etcd]# cd certs
[root@hdss7-128 certs]# ll		#把etcd的证书和私钥放在这个目录下
total 12
-rw-r--r-- 1 root root 1338 May 11 16:53 ca.pem
-rw------- 1 root root 1679 May 12 11:19 etcd-peer-key.pem	#注意私钥权限600
-rw-r--r-- 1 root root 1424 May 12 11:19 etcd-peer.pem
#启动etcd的脚本
[root@hdss7-128 etcd]# vi etcd-server-startup.sh 
#!/bin/sh
./etcd --name etcd-server-7-128 \
       --data-dir /data/etcd/etcd-server \
       --listen-peer-urls https://192.168.56.128:2380 \
       --listen-client-urls https://192.168.56.128:2379,http://1287.0.0.1:2379 \
       --quota-backend-bytes 8000000000 \
       --initial-advertise-peer-urls https://192.168.56.128:2380 \
       --advertise-client-urls https://192.168.56.128:2379,http://1287.0.0.1:2379 \
       --initial-cluster  etcd-server-7-128=https://192.168.56.128:2380,etcd-server-7-129=https://192.168.56.129:2380,etcd-server-7-130=https://192.168.56.130:2380 \
       --ca-file ./certss/ca.pem \
       --certs-file ./certss/etcd-peer.pem \
       --key-file ./certss/etcd-peer-key.pem \
       --client-certs-auth  \
       --trusted-ca-file ./certss/ca.pem \
       --peer-ca-file ./certss/ca.pem \
       --peer-certs-file ./certss/etcd-peer.pem \
       --peer-key-file ./certss/etcd-peer-key.pem \
       --peer-client-certs-auth \
       --peer-trusted-ca-file ./certss/ca.pem \
       --log-output stdout
[root@hdss7-128 etcd]# chmod +x etcd-server-startup.sh 
#更改属主
[root@hdss7-128 etcd]# chown -R etcd:etcd /opt/etcd-v3.1.20/
[root@hdss7-128 etcd]# chown -R etcd:etcd /data/etcd/
[root@hdss7-128 etcd]# chown -R etcd:etcd /data/logs/etcd-server/
#安装管理后台进程的软件
yum install supervisor -y
systemctl start supervisord 
systemctl enable supervisord
[root@hdss7-128 etcd]# vi /etc/supervisord.d/etcd-server.ini
[program:etcd-server-7-128]
command=/opt/etcd/etcd-server-startup.sh                        ; the program (relative uses PATH, can take args)
numprocs=1                                                      ; number of processes copies to start (def 1)
directory=/opt/etcd                                             ; directory to cwd to before exec (def no cwd)
autostart=true                                                  ; start at supervisord start (default: true)
autorestart=true                                                ; retstart at unexpected quit (default: true)
startsecs=30                                                    ; number of secs prog must stay running (def. 1)
startretries=3                                                  ; max # of serial start failures (default 3)
exitcodes=0,2                                                   ; 'expected' exit codes for process (default 0,2)
stopsignal=QUIT                                                 ; signal used to kill process (default TERM)
stopwaitsecs=10                                                 ; max num secs to wait b4 SIGKILL (default 10)
user=etcd                                                       ; setuid to this UNIX account to run the program
redirect_stderr=true                                            ; redirect proc stderr to stdout (default false)
stdout_logfile=/data/logs/etcd-server/etcd.stdout.log           ; stdout log path, NONE for none; default AUTO
stdout_logfile_maxbytes=64MB                                    ; max # logfile bytes b4 rotation (default 50MB)
stdout_logfile_backups=4                                        ; # of stdout logfile backups (default 10)
stdout_capture_maxbytes=1MB                                     ; number of bytes in 'capturemode' (default 0)
stdout_events_enabled=false                                     ; emit events on stdout writes (default false)
[root@hdss7-128 etcd]# supervisorctl update
etcd-server-7-128: added process group
[root@hdss7-128 etcd]# supervisorctl status
etcd-server-7-128                STARTING  
[root@hdss7-128 etcd]# chown etcd:etcd /data/etcd/etcd-server/ -R	
[root@hdss7-128 member]# netstat -luntp|grep etcd
tcp        0      0 192.168.56.128:2379     0.0.0.0:*               LISTEN      20906/./etcd        
tcp        0      0 127.0.0.1:2379          0.0.0.0:*               LISTEN      20906/./etcd        
tcp        0      0 192.168.56.128:2380     0.0.0.0:*               LISTEN      20906/./etcd  
#其他俩台机器配置跟上面一样，就是etcd-server-startup.sh( --initial-cluster不需要改) ，/etc/supervisord.d/etcd-server.ini 略有改动
```
##### 查看集群的健康状态
```
[root@hdss7-130 etcd]# ./etcdctl cluster-health
member 3d34a470df8f1443 is healthy: got healthy result from http://127.0.0.1:2379
member bef04d370192e0b0 is healthy: got healthy result from http://127.0.0.1:2379
member d7c4f71ecbf42673 is healthy: got healthy result from http://127.0.0.1:2379
cluster is healthy
[root@hdss7-130 etcd]# ./etcdctl member list	#查看集群健康状态，可以看到https://192.168.56.128:2380 是isLeader
3d34a470df8f1443: name=etcd-server-7-130 peerURLs=https://192.168.56.130:2380 clientURLs=http://127.0.0.1:2379,https://192.168.56.130:2379 isLeader=false
bef04d370192e0b0: name=etcd-server-7-128 peerURLs=https://192.168.56.128:2380 clientURLs=http://127.0.0.1:2379,https://192.168.56.128:2379 isLeader=true
d7c4f71ecbf42673: name=etcd-server-7-129 peerURLs=https://192.168.56.129:2380 clientURLs=http://127.0.0.1:2379,https://192.168.56.129:2379 isLeader=false
```
+ ### 部署kube-apiserver集群
https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.15.md#downloads-for-v11512  找到Server Binaries 下载kubernetes-server-linux-amd64.tar.gz

#### 1)安装准备
```
[root@hdss7-130 src]# tar xf kubernetes-server-linux-amd64-v1.15.2.tar.gz -C /opt/
[root@hdss7-130 opt]# ls
containerd  etcd  etcd-v3.1.20  kubernetes  src
[root@hdss7-130 opt]# mv kubernetes/ kubernetes-v1.15.2
[root@hdss7-130 opt]# ll
total 0
drwx--x--x. 4 root root  28 May 11 12:06 containerd
lrwxrwxrwx  1 root root  18 May 12 15:22 etcd -> /opt/etcd-v3.1.20/
drwxr-xr-x  4 etcd etcd 166 May 12 15:47 etcd-v3.1.20
drwxr-xr-x  4 root root  79 Aug  5  2019 kubernetes-v1.15.2
drwxr-xr-x  2 root root  97 May 12 16:35 src
[root@hdss7-130 opt]# ln -s /opt/kubernetes-v1.15.2/ /opt/kubernetes
[root@hdss7-130 opt]# cd kubernetes
[root@hdss7-130 kubernetes]# ll
total 27184
drwxr-xr-x 2 root root        6 Aug  5  2019 addons
-rw-r--r-- 1 root root 26625140 Aug  5  2019 kubernetes-src.tar.gz
-rw-r--r-- 1 root root  1205293 Aug  5  2019 LICENSES
drwxr-xr-x 3 root root       17 Aug  5  2019 server
[root@hdss7-130 kubernetes]# rm -rf kubernetes-src.tar.gz 
[root@hdss7-130 kubernetes]# cd server/bin/
[root@hdss7-130 bin]# rm -rf *.tar
[root@hdss7-130 bin]# rm -f *_tag
[root@hdss7-130 bin]# ll
total 884636
-rwxr-xr-x 1 root root  43534816 Aug  5  2019 apiextensions-apiserver
-rwxr-xr-x 1 root root 100548640 Aug  5  2019 cloud-controller-manager
-rwxr-xr-x 1 root root 200648416 Aug  5  2019 hyperkube
-rwxr-xr-x 1 root root  40182208 Aug  5  2019 kubeadm
-rwxr-xr-x 1 root root 164501920 Aug  5  2019 kube-apiserver
-rwxr-xr-x 1 root root 116397088 Aug  5  2019 kube-controller-manager
-rwxr-xr-x 1 root root  42985504 Aug  5  2019 kubectl
-rwxr-xr-x 1 root root 119616640 Aug  5  2019 kubelet
-rwxr-xr-x 1 root root  36987488 Aug  5  2019 kube-proxy
-rwxr-xr-x 1 root root  38786144 Aug  5  2019 kube-scheduler
-rwxr-xr-x 1 root root   1648224 Aug  5  2019 mounter
```
#### 2)签发(client)证书(apiserver)

##### 2.1)#在hdss7-132主机上
```
[root@hdss7-132 certs]# vi  /opt/certss/client-csr.json

{
    "CN": "k8s-node",
    "hosts": [
    ],
    "key": {
        "algo": "rsa",
        "size": 2048
    },
    "names": [
        {
            "C": "CN",
            "ST": "beijing",
            "L": "beijing",
            "O": "od",
            "OU": "ops"
        }
    ]
}
[root@hdss7-132 certs]# cfssl gencerts -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=client client-csr.json |cfssl-json -bare client
```
##### 2.2)创建生成证书签名请求的(csr)的json文件
```
[root@hdss7-132 certs]# vi /opt/certss/apiserver-csr.json

{
    "CN": "k8s-apiserver",
    "hosts": [
        "127.0.0.1",
        "192.168.0.1",
        "kubernetes.default",
        "kubernetes.default.svc",
        "kubernetes.default.svc.cluster",
        "kubernetes.default.svc.cluster.local",
        "192.168.56.128",
        "192.168.56.129",
        "192.168.56.130",
        "192.168.56.132"
    ],
    "key": {
        "algo": "rsa",
        "size": 2048
    },
    "names": [
        {
            "C": "CN",
            "ST": "beijing",
            "L": "beijing",
            "O": "od",
            "OU": "ops"
        }
    ]
}
```
##### 2.3)把创建好的证书发到hdss7-130主机上
```
[root@hdss7-130 certs]# pwd
/opt/kubernetes/server/bin/certs
[root@hdss7-130 certs]# ll
total 24
-rw------- 1 root root 1679 May 12 17:16 apiserver-key.pem
-rw-r--r-- 1 root root 1594 May 12 17:16 apiserver.pem
-rw------- 1 root root 1679 May 11 16:53 ca-key.pem
-rw-r--r-- 1 root root 1338 May 11 16:53 ca.pem
-rw------- 1 root root 1679 May 12 17:06 client-key.pem
-rw-r--r-- 1 root root 1363 May 12 17:06 client.pem
```
##### 2.4)创建apiserver需要的配置文件
```
[root@hdss7-130 bin]# mkdir conf & cd conf
[root@hdss7-130 conf]# pwd
/opt/kubernetes/server/bin/conf
[root@hdss7-130 conf]# vim /opt/kubernetes/server/bin/conf/audit.yaml	#创建审计策略文件
apiVersion: audit.k8s.io/v1beta1 # This is required.
kind: Policy
# Don't generate audit events for all requests in RequestReceived stage.
omitStages:
  - "RequestReceived"
rules:
  # Log pod changes at RequestResponse level
  - level: RequestResponse
    resources:
    - group: ""
      # Resource "pods" doesn't match requests to any subresource of pods,
      # which is consistent with the RBAC policy.
      resources: ["pods"]
  # Log "pods/log", "pods/status" at Metadata level
  - level: Metadata
    resources:
    - group: ""
      resources: ["pods/log", "pods/status"]

  # Don't log requests to a configmap called "controller-leader"
  - level: None
    resources:
    - group: ""
      resources: ["configmaps"]
      resourceNames: ["controller-leader"]

  # Don't log watch requests by the "system:kube-proxy" on endpoints or services
  - level: None
    users: ["system:kube-proxy"]
    verbs: ["watch"]
    resources:
    - group: "" # core API group
      resources: ["endpoints", "services"]

  # Don't log authenticated requests to certsain non-resource URL paths.
  - level: None
    userGroups: ["system:authenticated"]
    nonResourceURLs:
    - "/api*" # Wildcard matching.
    - "/version"

  # Log the request body of configmap changes in kube-system.
  - level: Request
    resources:
    - group: "" # core API group
      resources: ["configmaps"]
    # This rule only applies to resources in the "kube-system" namespace.
    # The empty string "" can be used to select non-namespaced resources.
    namespaces: ["kube-system"]

  # Log configmap and secret changes in all other namespaces at the Metadata level.
  - level: Metadata
    resources:
    - group: "" # core API group
      resources: ["secrets", "configmaps"]

  # Log all other resources in core and extensions at the Request level.
  - level: Request
    resources:
    - group: "" # core API group
    - group: "extensions" # Version of group should NOT be included.

  # A catch-all rule to log all other requests at the Metadata level.
  - level: Metadata
    # Long-running requests like watches that fall under this rule will not
    # generate an audit event in RequestReceived.
    omitStages:
      - "RequestReceived"
[root@hdss7-130 bin]# vim /opt/kubernetes/server/bin/kube-apiserver.sh	#创建启动apiserver脚本
#!/bin/bash
./kube-apiserver \
  --apiserver-count 2 \
  --audit-log-path /data/logs/kubernetes/kube-apiserver/audit-log \
  --audit-policy-file ./conf/audit.yaml \
  --authorization-mode RBAC \
  --client-ca-file ./certs/ca.pem \
  --requestheader-client-ca-file ./certs/ca.pem \
  --enable-admission-plugins NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota \
  --etcd-cafile ./certs/ca.pem \
  --etcd-certfile ./certs/client.pem \
  --etcd-keyfile ./certs/client-key.pem \  --etcd-servers https://192.168.56.128:2379,https://192.168.56.129:2379,https://192.168.56.130:2379 \
  --service-account-key-file ./certs/ca-key.pem \
  --service-cluster-ip-range 192.168.0.0/16 \
  --service-node-port-range 3000-29999 \
  --target-ram-mb=1024 \
  --kubelet-client-certificate ./certs/client.pem \
  --kubelet-client-key ./certs/client-key.pem \
  --log-dir  /data/logs/kubernetes/kube-apiserver \
  --tls-cert-file ./certs/apiserver.pem \
  --tls-private-key-file ./certs/apiserver-key.pem \
  --v 2
[root@hdss7-130 bin]# chmod +x kube-apiserver.sh
[root@hdss7-130 bin]# mkdir -p /data/logs/kubernetes/kube-apiserver
[root@hdss7-130 bin]# vi /etc/supervisord.d/kube-apiserver.ini	#配置supervisor后台管理
[program:kube-apiserver-7-130]		#kube-apiserver-7-130 130 129 128 配置对应的主机ip后一位
command=/opt/kubernetes/server/bin/kube-apiserver.sh            ; the program (relative uses PATH, can take args)
numprocs=1                                                      ; number of processes copies to start (def 1)
directory=/opt/kubernetes/server/bin                            ; directory to cwd to before exec (def no cwd)
autostart=true                                                  ; start at supervisord start (default: true)
autorestart=true                                                ; retstart at unexpected quit (default: true)
startsecs=30                                                    ; number of secs prog must stay running (def. 1)
startretries=3                                                  ; max # of serial start failures (default 3)
exitcodes=0,2                                                   ; 'expected' exit codes for process (default 0,2)
stopsignal=QUIT                                                 ; signal used to kill process (default TERM)
stopwaitsecs=10                                                 ; max num secs to wait b4 SIGKILL (default 10)
user=root                                                       ; setuid to this UNIX account to run the program
redirect_stderr=true                                            ; redirect proc stderr to stdout (default false)
stdout_logfile=/data/logs/kubernetes/kube-apiserver/apiserver.stdout.log        ; stderr log path, NONE for none; default AUTO
stdout_logfile_maxbytes=64MB                                    ; max # logfile bytes b4 rotation (default 50MB)
stdout_logfile_backups=4                                        ; # of stdout logfile backups (default 10)
stdout_capture_maxbytes=1MB                                     ; number of bytes in 'capturemode' (default 0)
stdout_events_enabled=false                                     ; emit events on stdout writes (default false)
```
##### 验证
```
[root@hdss7-128 certs]# supervisorctl status
etcd-server-7-128                RUNNING   pid 22474, uptime 0:07:25
kube-apiserver-7-128             RUNNING   pid 22473, uptime 0:07:25
[root@hdss7-129 bin]# supervisorctl status
etcd-server-7-129                RUNNING   pid 24018, uptime 0:10:53
kube-apiserver-7-129             RUNNING   pid 24017, uptime 0:10:53
[root@hdss7-129 bin]#
[root@hdss7-130 conf]# supervisorctl status
etcd-server-7-130                RUNNING   pid 24014, uptime 0:15:44
kube-apiserver-7-130             RUNNING   pid 24015, uptime 0:15:44
[root@hdss7-130 conf]# netstat -lntup |grep api	 
tcp        0      0 127.0.0.1:8080          0.0.0.0:*               LISTEN      24017/./kube-apiser 	#监听本地回环地址上的8080端口
tcp6       0      0 :::6443                 :::*                    LISTEN      24017/./kube-apiser 	#监听6443端口
```
+ ### nginx四层反向代理

##### 在128 129机器上安装nginx
```
yum install nginx -y
[root@hdss7-128 etcd]# vim /etc/nginx/nginx.conf #在nginx主配置文件最后配置以下字段
[root@hdss7-129 bin]# vim /etc/nginx/nginx.conf	#在nginx主配置文件最后配置以下字段
stream {
    upstream kube-apiserver {
        server 192.168.56.130:6443     max_fails=3 fail_timeout=30s;
    }
    server {
        listen 7443;
        proxy_connect_timeout 2s;
        proxy_timeout 900s;
        proxy_pass kube-apiserver;
    }
}
```
##### 在128 129机器上安装keepalived
```
yum install keepalived -y		#128,129都安装
[root@hdss7-128 etcd]# vim /etc/keepalived/check_port.sh	#129也添加脚本
#!/bin/bash
CHK_PORT=$1
if [ -n "$CHK_PORT" ];then
        PORT_PROCESS=`ss -lnt|grep $CHK_PORT|wc -l`
        if [ $PORT_PROCESS -eq 0 ];then
                echo "Port $CHK_PORT Is Not Used,End."
                exit 1
        fi
else
        echo "Check Port Cant Be Empty!"
fi
[root@hdss7-128 etcd]# chmod +x /etc/keepalived/check_port.sh
```
##### keepalived 主:
```
[root@hdss7-128 ~]# vim /etc/keepalived/keepalived.conf 

! Configuration File for keepalived

global_defs {
   router_id 192.168.56.128

}

vrrp_script chk_nginx {
    script "/etc/keepalived/check_port.sh 7443"
    interval 2
    weight -20
}
vrrp_instance VI_1 {
    state MASTER
    interface ens33
    virtual_router_id 251
    priority 100
    advert_int 1
    mcast_src_ip 192.168.56.128
    nopreempt	#非强占式，这样配置vip不会自动回到keepalived上，
    
    authentication {
        auth_type PASS
        auth_pass 11111111
    }   
    track_script {
         chk_nginx
    }    
    virtual_ipaddress {
        192.168.56.132
    }   
} 
[root@hdss7-128 ~]# systemctl start keepalived
[root@hdss7-128 ~]# systemctl enable  keepalived
```
##### keepalived从:
```
[root@hdss7-129 bin]# vim  /etc/keepalived/keepalived.conf 

! Configuration File for keepalived
global_defs {
        router_id 192.168.56.129
}
vrrp_script chk_nginx {
        script "/etc/keepalived/check_port.sh 7443"
        interval 2
        weight -20
}
vrrp_instance VI_1 {
        state BACKUP
        interface ens33
        virtual_router_id 251
        mcast_src_ip 192.168.56.129
        priority 90
        advert_int 1
        authentication {
                auth_type PASS
                auth_pass 11111111
        }
        track_script {
                chk_nginx
        }
        virtual_ipaddress {
                192.168.56.132
        }
}
[root@hdss7-129 bin]# systemctl start keepalived
[root@hdss7-129 bin]# systemctl enable  keepalived
```
##### 验证keepalived代理是否成功
```
[root@hdss7-128 ~]# ip add		#在keepalived主上验证
...
2: ens33: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 00:0c:29:ab:43:5d brd ff:ff:ff:ff:ff:ff
    inet 192.168.56.128/24 brd 192.168.56.255 scope global noprefixroute dynamic ens33
       valid_lft 1743sec preferred_lft 1743sec
    inet 192.168.56.132/32 scope global ens33		#可以看到keepalived代理成功
       valid_lft forever preferred_lft forever
.....
[root@hdss7-128 ~]# nginx -s stop	#在keepal主上停掉nginx，nginx四层代理也就停了
[root@hdss7-128 ~]# netstat -lnutp|grep 7443
[root@hdss7-129 /]# ip add		#在keepalived从上查看
...
2: ens33: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 00:0c:29:72:cb:12 brd ff:ff:ff:ff:ff:ff
    inet 192.168.56.129/24 brd 192.168.56.255 scope global noprefixroute dynamic ens33
       valid_lft 1359sec preferred_lft 1359sec
    inet 192.168.56.132/32 scope global ens33		#可以看到飘过来了哦
       valid_lft forever preferred_lft forever
...
```
##### 在129和130机器上
```
#配置kube-controller-manager
vim /opt/kubernetes/server/bin/kube-controller-manager.sh
#!/bin/sh
./kube-controller-manager \
  --cluster-cidr 172.7.0.0/16 \
  --leader-elect true \
  --log-dir /data/logs/kubernetes/kube-controller-manager \
  --master http://127.0.0.1:8080 \
  --service-account-private-key-file ./cert/ca-key.pem \
  --service-cluster-ip-range 192.168.0.0/16 \
  --root-ca-file ./cert/ca.pem \
  --v 2

chmod +x /opt/kubernetes/server/bin/kube-controller-manager.sh
mkdir -p /data/logs/kubernetes/kube-controller-manager
vim /etc/supervisord.d/kube-conntroller-manager.ini
[program:kube-controller-manager-7-130]
command=/opt/kubernetes/server/bin/kube-controller-manager.sh                     ; the program (relative uses PATH, can take args)
numprocs=1                                                                        ; number of processes copies to start (def 1)
directory=/opt/kubernetes/server/bin                                              ; directory to cwd to before exec (def no cwd)
autostart=true                                                                    ; start at supervisord start (default: true)
autorestart=true                                                                  ; retstart at unexpected quit (default: true)
startsecs=30                                                                      ; number of secs prog must stay running (def. 1)
startretries=3                                                                    ; max # of serial start failures (default 3)
exitcodes=0,2                                                                     ; 'expected' exit codes for process (default 0,2)
stopsignal=QUIT                                                                   ; signal used to kill process (default TERM)
stopwaitsecs=10                                                                   ; max num secs to wait b4 SIGKILL (default 10)
user=root                                                                         ; setuid to this UNIX account to run the program
redirect_stderr=true                                                              ; redirect proc stderr to stdout (default false)
stdout_logfile=/data/logs/kubernetes/kube-controller-manager/controller.stdout.log  ; stderr log path, NONE for none; default AUTO
stdout_logfile_maxbytes=64MB                                                      ; max # logfile bytes b4 rotation (default 50MB)
stdout_logfile_backups=4                                                          ; # of stdout logfile backups (default 10)
stdout_capture_maxbytes=1MB                                                       ; number of bytes in 'capturemode' (default 0)
stdout_events_enabled=false                                                       ; emit events on stdout writes (default false)

supervisorctl update
```
##### 配置kube-scheduler
```
vim /opt/kubernetes/server/bin/kube-scheduler.sh
#!/bin/sh
./kube-scheduler \
  --leader-elect  \
  --log-dir /data/logs/kubernetes/kube-scheduler \
  --master http://127.0.0.1:8080 \
  --v 2

chmod +x /opt/kubernetes/server/bin/kube-scheduler.sh
mkdir -p /data/logs/kubernetes/kube-scheduler

vim /etc/supervisord.d/kube-scheduler.ini
[program:kube-scheduler-7-130]
command=/opt/kubernetes/server/bin/kube-scheduler.sh                     ; the program (relative uses PATH, can take args)
numprocs=1                                                               ; number of processes copies to start (def 1)
directory=/opt/kubernetes/server/bin                                     ; directory to cwd to before exec (def no cwd)
autostart=true                                                           ; start at supervisord start (default: true)
autorestart=true                                                         ; retstart at unexpected quit (default: true)
startsecs=30                                                             ; number of secs prog must stay running (def. 1)
startretries=3                                                           ; max # of serial start failures (default 3)
exitcodes=0,2                                                            ; 'expected' exit codes for process (default 0,2)
stopsignal=QUIT                                                          ; signal used to kill process (default TERM)
stopwaitsecs=10                                                          ; max num secs to wait b4 SIGKILL (default 10)
user=root                                                                ; setuid to this UNIX account to run the program
redirect_stderr=true                                                     ; redirect proc stderr to stdout (default false)
stdout_logfile=/data/logs/kubernetes/kube-scheduler/scheduler.stdout.log ; stderr log path, NONE for none; default AUTO
stdout_logfile_maxbytes=64MB                                             ; max # logfile bytes b4 rotation (default 50MB)
stdout_logfile_backups=4                                                 ; # of stdout logfile backups (default 10)
stdout_capture_maxbytes=1MB                                              ; number of bytes in 'capturemode' (default 0)
stdout_events_enabled=false                                              ; emit events on stdout writes (default false)

[root@hdss7-129 kubernetes]# supervisorctl status
etcd-server-7-129                RUNNING   pid 1024, uptime 1:18:51
kube-apiserver-7-129             RUNNING   pid 1909, uptime 1:18:27
kube-controller-manager-7-129    RUNNING   pid 16376, uptime 0:01:00
kube-scheduler-7-129             RUNNING   pid 16132, uptime 0:02:13
```
##### 验证apiserver集群健康状态
```
`ln -s /opt/kubernetes/server/bin/kubectl /usr/bin/kubectl`
[root@hdss7-130 /]# kubectl get cs
NAME                 STATUS    MESSAGE              ERROR
controller-manager   Healthy   ok                   
scheduler            Healthy   ok                   
etcd-1               Healthy   {"health": "true"}   
etcd-0               Healthy   {"health": "true"}   
etcd-2               Healthy   {"health": "true"}  
```
+ ### 部署运算节点服务kubelet

##### 1)部署node节点服务(129-130),签发证书
```
#在132机器上
vim /opt/certs/kubelet-csr.json
cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=server kubelet-csr.json | cfssl-json -bare kubelet
#把认证好的证书kubelet.pem，kubelet-key.pem放在/opt/kubernetes/server/bin/certs目录下，私钥权限600
```
##### 2)创建配置(/opt/kubernetes/server/bin/conf)
##### 2.1)set-cluster
```
[root@hdss7-129 conf]# kubectl config set-cluster myk8s \
   --certificate-authority=/opt/kubernetes/server/bin/certs/ca.pem \
   --embed-certs=true \
   --server=https://192.168.56.133:7443 \
   --kubeconfig=kubelet.kubeconfig
Cluster "myk8s" set.
[root@hdss7-129 conf]# ls
audit.yaml  kubelet.kubeconfig
```
##### 2.2)set-credentials
```
[root@hdss7-130 conf]# kubectl config set-credentials k8s-node \
   --client-certificate=/opt/kubernetes/server/bin/certs/client.pem \
   --client-key=/opt/kubernetes/server/bin/certs/client-key.pem \
   --embed-certs=true \
   --kubeconfig=kubelet.kubeconfig
User "k8s-node" set.
[root@hdss7-130 conf]# ls
audit.yaml  kubelet.kubeconfig
```
##### 2.3)set-context
```
[root@hdss7-130 conf]# kubectl config set-context myk8s-context \
   --cluster=myk8s \
   --user=k8s-node \
   --kubeconfig=kubelet.kubeconfig
Context "myk8s-context" created.
```
##### 2.4)use-context 
```
[root@hdss7-130 conf]# kubectl config use-context myk8s-context --kubeconfig=kubelet.kubeconfig	#切换上下文到k8s-node
Switched to context "myk8s-context".
```
##### 2.5)创建
```
[root@hdss7-130 conf]# vim /opt/kubernetes/server/bin/conf/k8s-node.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: k8s-node
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: system:node
subjects:
- apiGroup: rbac.authorization.k8s.io
  kind: User
  name: k8s-node
[root@hdss7-129 conf]# kubectl create -f k8s-node.yaml 
clusterrolebinding.rbac.authorization.k8s.io/k8s-node created
[root@hdss7-129 conf]# kubectl get clusterrolebinding k8s-node -o yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  creationTimestamp: "2020-05-13T06:26:32Z"
  name: k8s-node
  resourceVersion: "16968"
  selfLink: /apis/rbac.authorization.k8s.io/v1/clusterrolebindings/k8s-node
  uid: d66c735e-6b50-4a15-a785-ee1e1fb47b1a
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: system:node
subjects:
- apiGroup: rbac.authorization.k8s.io
  kind: User
  name: k8s-node

```
##### 3)准备pause基础镜像

###### 在安装harbor的主机上下载镜像
```
docker pull kubernetes/pause
docker tag f9d5de079539 harbor.od.com/public/pause:latest
docker push harbor.od.com/public/pause:latest
```
##### 可以看大harbor上的，pause镜像
[](https://github.com/Kingserch/Job-accumulation/blob/Kubernetes/images/harbor1.png)

##### 4)创建kubelet的启动脚本

##### 4.1)创建129-130主机上启动脚本
```
[root@hdss7-130 conf]# vim /opt/kubernetes/server/bin/kubelet.sh
#!/bin/sh
./kubelet \
  --anonymous-auth=false \		#不允许匿名登录，必须经过apiserver才能指挥kublet干活
  --cgroup-driver systemd \		#跟docker  cgroup-driver保持一致
  --cluster-dns 192.168.0.2 \
  --cluster-domain cluster.local \
  --runtime-cgroups=/systemd/system.slice \
  --kubelet-cgroups=/systemd/system.slice \
  --fail-swap-on="false" \		#运算节点没关闭交换分区时候，不使用交换分区
  --client-ca-file ./certs/ca.pem \
  --tls-cert-file ./certs/kubelet.pem \
  --tls-private-key-file ./certs/kubelet-key.pem \
  --hostname-override hdss7-130.host.com \		#hostname-override要根据主机的名字来改
  --image-gc-high-threshold 20 \
  --image-gc-low-threshold 10 \
  --kubeconfig ./conf/kubelet.kubeconfig \
  --log-dir /data/logs/kubernetes/kube-kubelet \
  --pod-infra-container-image harbor.od.com/public/pause:latest \	#网络空间，UTS空间，IPC空间由pod做初始化的
  --root-dir /data/kubelet
  
mkdir -p /data/logs/kubernetes/kube-kubelet/data/kubelet	#创建存放日志的文件的目录
chmod +x  /opt/kubernetes/server/bin/kubelet.sh
```
##### 4.2)创建supervisor配置
```
[root@hdss7-130 conf]# vim /etc/supervisord.d/kube-kubelet.ini
[program:kube-kubelet-7-130]
command=/opt/kubernetes/server/bin/kubelet.sh     ; the program (relative uses PATH, can take args)
numprocs=1                                        ; number of processes copies to start (def 1)
directory=/opt/kubernetes/server/bin              ; directory to cwd to before exec (def no cwd)
autostart=true                                    ; start at supervisord start (default: true)
autorestart=true              		          ; retstart at unexpected quit (default: true)
startsecs=30                                      ; number of secs prog must stay running (def. 1)
startretries=3                                    ; max # of serial start failures (default 3)
exitcodes=0,2                                     ; 'expected' exit codes for process (default 0,2)
stopsignal=QUIT                                   ; signal used to kill process (default TERM)
stopwaitsecs=10                                   ; max num secs to wait b4 SIGKILL (default 10)
user=root                                         ; setuid to this UNIX account to run the program
redirect_stderr=true                              ; redirect proc stderr to stdout (default false)
stdout_logfile=/data/logs/kubernetes/kube-kubelet/kubelet.stdout.log   ; stderr log path, NONE for none; default AUTO
stdout_logfile_maxbytes=64MB                      ; max # logfile bytes b4 rotation (default 50MB)
stdout_logfile_backups=4                          ; # of stdout logfile backups (default 10)
stdout_capture_maxbytes=1MB                       ; number of bytes in 'capturemode' (default 0)
stdout_events_enabled=false                       ; emit events on stdout writes (default false)
[root@hdss7-130 bin]# supervisorctl status
etcd-server-7-130                STARTING  
kube-apiserver-7-130             STARTING  
kube-controller-manager-7-130    STARTING  
kube-kubelet-7-130               STARTING  
kube-scheduler-7-130             STARTING 
```